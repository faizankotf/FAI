{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7kAjwPMIqVWzIS15MkgNJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faizankotf/FAI/blob/main/Untitled31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:**\n",
        "Write a function which takes in the base llama-2 model, freezes the pretrained layers, adds\n",
        "the adapter layer inside the llama-2 architecture and returns the updated architecture for\n",
        "further training."
      ],
      "metadata": {
        "id": "SgCNG3oc9Wc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "2xR2HlqA19yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaConfig\n",
        "\n",
        "def add_adapter_to_llama2(base_model, adapter_dim=64):\n",
        "    \"\"\"\n",
        "    Freezes the pretrained layers of the base Llama 2 model,\n",
        "    adds an adapter layer inside the architecture,\n",
        "    and returns the updated architecture for further training.\n",
        "\n",
        "    Args:\n",
        "        base_model: The base Llama 2 model.\n",
        "        adapter_dim: The dimension of the adapter layer.\n",
        "\n",
        "    Returns:\n",
        "        The updated Llama 2 model with the adapter layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # Freeze the pretrained layers\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Get the configuration of the base model\n",
        "    config = base_model.config\n",
        "\n",
        "    # Add the adapter layer\n",
        "    for layer in base_model.model.layers:\n",
        "        # Get the hidden size of the layer\n",
        "        hidden_size = layer.self_attn.embed_dim\n",
        "\n",
        "        # Create the adapter layer\n",
        "        adapter = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, adapter_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(adapter_dim, hidden_size),\n",
        "        )\n",
        "\n",
        "        # Insert the adapter layer after the self-attention layer\n",
        "        layer.self_attn.out_proj = torch.nn.Sequential(\n",
        "            layer.self_attn.out_proj, adapter\n",
        "        )\n",
        "\n",
        "    return base_model"
      ],
      "metadata": {
        "id": "1MUrLEM82tbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}